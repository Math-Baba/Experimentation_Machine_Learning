# -*- coding: utf-8 -*-
"""prob_Bayes_Mathieu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PezF_ht3Ctu5sb2lqcy_00JOd4nj5VkV
"""

# === Importations initiales ===
import pandas as pd
from google.colab import files
uploaded = files.upload()  # Pour charger un fichier depuis ton ordinateur (Colab)

# Charger le fichier Excel contenant les données énergétiques des bâtiments
file_path = "ENB2012_data.xlsx"
df = pd.read_excel(file_path)  # Lecture du fichier Excel dans un DataFrame

# Afficher les premières lignes du DataFrame pour vérifier le contenu
print(df.head())

# === Bibliothèques nécessaires ===
import numpy as np
import pandas as pd
from collections import defaultdict  # Pour stocker les stats des features par classe
from scipy.stats import norm         # Pour calculer la densité de probabilité normale

# === Fonction : implémentation manuelle du Naïf Bayes ===
def naive_bayes_manual(X_train, Y_train, X_test):
    """
    Implémente le classificateur Naïf Bayes manuellement en supposant une distribution normale pour chaque feature.
    """
    classes = np.unique(Y_train)  # Liste des classes uniques (0 à 4 après discrétisation)
    class_probs = Y_train.value_counts(normalize=True)  # P(A) : probabilité a priori de chaque classe
    feature_stats = defaultdict(dict)  # Dictionnaire pour stocker moyenne et écart-type par classe

    # Calculer la moyenne et l'écart-type pour chaque feature et chaque classe
    for class_value in classes:
        subset = X_train[Y_train == class_value]  # Filtrer les lignes correspondant à la classe
        feature_stats[class_value]["mean"] = subset.mean()
        feature_stats[class_value]["std"] = subset.std()

    predictions = []  # Liste finale des prédictions

    # Prédire la classe pour chaque échantillon du test
    for _, sample in X_test.iterrows():
        probabilities = {}

        for class_value in classes:
            # Probabilité a priori
            prob = class_probs[class_value]

            # Probabilités conditionnelles : produit des lois normales P(feature|class)
            for feature in X_train.columns:
                if feature_stats[class_value]["std"][feature] == 0:  # Éviter division par zéro
                    feature_stats[class_value]["std"][feature] = 1e-6  # Petitre valeur pour stabiliser

                # Densité de probabilité normale
                prob *= norm.pdf(sample[feature],
                                 feature_stats[class_value]["mean"][feature],
                                 feature_stats[class_value]["std"][feature])

            probabilities[class_value] = prob  # Stocker probabilité finale pour la classe

        # Sélection de la classe avec la plus haute probabilité
        predicted_class = max(probabilities, key=probabilities.get)
        predictions.append(predicted_class)

    return predictions  # Liste des classes prédites

# === Fonction : version Scikit-Learn de Naïf Bayes ===
from sklearn.naive_bayes import GaussianNB

def naive_bayes_sklearn(X_train, Y_train, X_test):
    """
    Implémente le classificateur Naïf Bayes en utilisant Scikit-Learn (GaussianNB).
    """
    model = GaussianNB()  # Création du modèle
    model.fit(X_train, Y_train.ravel())  # Entraînement du modèle sur les données
    return model.predict(X_test)  # Prédictions sur les données de test

# === Chargement et préparation des données ===
from sklearn.model_selection import train_test_split
from google.colab import files
uploaded = files.upload()  # (optionnel, répété ici)

# Relecture du fichier Excel
file_path = "ENB2012_data.xlsx"
df = pd.read_excel(file_path)

# Affichage pour vérification
print(df.head())

# === Séparation des variables ===
X = df.iloc[:, :-2]  # Variables explicatives (X1 à X8)
Y = df["Y1"]         # Variable cible : consommation de chauffage

# Séparation des données en train (80%) et test (20%)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# === Discrétisation de la variable Y (chauffage) ===
from sklearn.preprocessing import KBinsDiscretizer

discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')  # 5 classes équilibrées
Y_train_discrete = discretizer.fit_transform(Y_train.values.reshape(-1, 1)).astype(int)  # Apprentissage
Y_test_discrete = discretizer.transform(Y_test.values.reshape(-1, 1)).astype(int)       # Application sur test

# === Prédiction avec l'implémentation manuelle ===
pred_manual = naive_bayes_manual(X_train, Y_train_discrete.flatten(), X_test)
print(f"Prédictions manuelles : {pred_manual[:5]}")  # Affiche les 5 premières prédictions

# === Prédiction avec Scikit-Learn ===
pred_sklearn = naive_bayes_sklearn(X_train, Y_train_discrete, X_test)
print(f"Prédictions Scikit-Learn : {pred_sklearn[:5]}")  # Affiche les 5 premières prédictions
