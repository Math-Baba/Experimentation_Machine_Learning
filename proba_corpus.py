# -*- coding: utf-8 -*-
"""proba_corpus_Mathieu.ipy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uqni2l-obT_roQH1Zdei2HaI-oq0Q6Z_
"""

# Import des librairies
import re
import spacy
from google.colab import files

uploaded = files.upload()  # Permettra d'uploader le fichier manuellement


# Lecture du fichier texte
with open("PG.txt", "r", encoding="utf-8") as file:
    corpus = file.read()

# Nettoyage basique du texte
corpus = corpus.lower()  # Conversion en minuscules
corpus = re.sub(r'\W+', ' ', corpus)  # Suppression des caractères spéciaux
corpus = re.sub(r'\s+', ' ', corpus).strip()  # Suppression des espaces inutiles


# Affichage des premières lignes pour vérifier
print(corpus[:500])  # Afficher un aperçu


!python -m spacy download fr_core_news_sm  # Téléchargement du modèle en français
nlp = spacy.load("fr_core_news_sm") # Modèle pré-entraîné en français
doc = nlp(corpus)  # Transformation en objets NLP

tokens = [token.text for token in doc if not token.is_stop and not token.is_punct and not token.like_num]  # Suppression des stopwords, chiffres et ponctuation

print(tokens[:20])  # Afficher les premiers tokens

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Convertir la liste de tokens en une grande chaîne de texte
text_cleaned = " ".join(tokens)

# Créer le nuage de mots
wordcloud = WordCloud(width=800, height=400, background_color="white", colormap="viridis").generate(text_cleaned)

# Affichage du nuage de mots
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")  # Masquer les axes
plt.show()

from collections import defaultdict, Counter
from nltk.util import ngrams
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')  # Pour la tokenisation

#Probabilités unigramme P(w)
unigram_counts=Counter(tokens)
total_tokens=sum(unigram_counts.values())

def unigram_prob(word):
  return unigram_counts[word]/total_tokens

print("Probabilités du mot 'bien' :", unigram_prob('bien'))

bigrams = list(ngrams(tokens, 2))
print("Liste des bigrammes :")
for bg in bigrams:
    print(bg)

#Compte des bigrammes
bigram_counts = Counter(ngrams(tokens, 2))

# Probabilité conditionnelle P(w2 | w1) = Count(w1, w2) / Count(w1)
def bigram_prob(w1, w2):
    return bigram_counts[(w1, w2)] / unigram_counts[w1]


print("\nProbabilité bigramme P('étrennes' | 'germaine'):", bigram_prob('germaine', 'étrennes'))

subset_text = " ".join(tokens[:5000])
trigrams = list(ngrams(tokens, 3))
print("Liste des trigrammes :")
for tr in trigrams:
    print(tr)

#Compte des trigrammes
trigram_counts = Counter(ngrams(tokens, 3))
bigram_counts_for_trigrams = Counter(ngrams(tokens, 3))  # Utilisé pour P(w3 | w1, w2)

#Probabilité conditionnelle P(w3 | w1, w2) = Count(w1, w2, w3) / Count(w1, w2)
def trigram_prob_safe(w1, w2, w3):
    if bigram_counts_for_trigrams[(w1, w2)] == 0:
        return 1e-10  # Retourne une très petite probabilité au lieu de 0
    return trigram_counts[(w1, w2, w3)] / bigram_counts_for_trigrams[(w1, w2)]


print("\nProbabilité trigramme P('duchesse' | 'germaine', 'étrennes'):", trigram_prob_safe('germaine', 'étrennes', 'duchesse'))

#Calcule la probabilité d'une phrase avec les probabilités trigrammes conditionnelles.
def sentence_probability(sentence):
    words = sentence.lower().split()  # Tokenisation basique
    prob = 1.0  # Initialisation à 1 pour la multiplication

    #Vérifier si la phrase a au moins 3 mots
    if len(words) < 3:
        return 0  #Trop courte pour un trigramme

    #Parcourir les trigrammes et calculer les probabilités conditionnelles
    for i in range(len(words) - 2):
        w1, w2, w3 = words[i], words[i + 1], words[i + 2]
        prob *= trigram_prob_safe(w1, w2, w3)

    return prob

#Exemple:
sentence = "une habitation de noble apparence"
print("Probabilité de la phrase:", sentence_probability(sentence))